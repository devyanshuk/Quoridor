\chapter{Experiments} \label{Experiments}

In this chapter, we define our experiment, the parameters involved in it for different agents and implementations and the final results of the simulation based on the implementation. The main focus of this chapter will be to characterize the complexity for different implementations mainly in terms of move time complexity with different implementations and the result of tournament considered between the different agents.

We first consider the implementation of the Minimax algorithm with the depth limited version, alpha-beta pruning version and the parallel version for different Quoridor board dimensions and demonstrate the time complexity of the implemented agents in terms of average move time complexity.\\
To run the experiment on all 3 implemented variants of the Minimax algorithm, we use the \textbf{Quoridor.ConsoleApp} console application, and simulate 100 games with different board and agent configurations against the semi-random strategy - a total of 36 times (3 variants of minimax x 3 depth x 4 dimensions).

For example, we use the following command line arguments to simulate 100 games between \textbf{Parallel Minimax} with a depth of 1 and \textbf{Semi-Random} agents on a game board of dimension 5x5:

\begin{lstlisting}[language=bash]
$dotnet run play -s1=parallelminimaxab -s2=semirandom -dimension=5 -depth=1 -sim -numsim=100
\end{lstlisting}

The aforementioned command produces the following output:

\begin{lstlisting}
===Results===
Player A : Parallel MinimaxAlgorithmABPruning won 99/100 games. Win rate : 99%. Average move time(ms) : 3.04
Player B : Semi-Random won 1/100 games. Win rate : 1%. Average move time(ms) : 1.05
Toal moves made across 100 games : 1781
Average total moves per game : 17
\end{lstlisting}

From the result above, we see that Parallel Minimax took \textbf{1781} moves across 100 games, with an average of \textbf{3.04 ms} per move.

\begin{figure}[!ht]
    \centering
    \includegraphics[width=\linewidth]{../img/performance.png}
    \caption{Average move time comparison between all 3 Minimax variants for different board sizes}
    \label{fig:minimax_performance_comp}
\end{figure}

In the figure \ref{fig:minimax_performance_comp}, we can see the average move time complexity of the implementation with different board dimension. Considering the board of large dimension (e.g., 7x7 and 9x9), without further optimization, the complexity of implementing the algorithm for Quoridor is prohibitively high. For example, for depth 3, the move time complexity for the board with dimension 7 is 82 seconds and that for dimension 9 is 244 seconds.

From the figure, it can be inferred that for depth 1, the minimax algorithm without any further optimizations including alpha-beta or parallel implementation performs the best. The reason for this is straightforward, as for depth 1, the algorithm only evaluates one further move. In such case, there is not further improvement with alpha beta as we anyways need to evaluate all the nodes. Similarly, the complexity of implementing the parallel algorithm outweighs the simple sequential evaluation.

However, for depths more than 1, it can be seen that the time complexity for each move considering alpha-beta pruning and further parallel implementation of the algorithm significantly improves as we increase the considered depth. Below, we have a table showing the improvement of the time complexity considering the minimax algorithm without any further implementation as baseline.

\begin{table}[ht]
    \centering
     \begin{tabular}{|c|c|c|c|}\hline
          & Depth 1 & Depth 2 & Depth 3\\ \hline 
          \textbf{Dimension: 3x3}  &    &    &     \\ \hline
          Minimax  &  1  &   1 &   1  \\ \hline
          Minimax alpha-beta &  1.8 &  0.92 &  0.61 \\ \hline
          Minimax alpha-beta parallel & 1.79 & 0.8& 0.29  \\ \hline
          \textbf{Dimension: 5x5}  &    &    &     \\ \hline
          Minimax  &  1  &   1 &   1  \\ \hline
          Minimax alpha-beta &  1.82 &  0.72 &  0.23 \\ \hline
          Minimax alpha-beta parallel & 2.06 & 0.27 & 0.20  \\ \hline
          \textbf{Dimension: 7x7}  &    &    &     \\ \hline
          Minimax  &  1  &   1 &   1  \\ \hline
          Minimax alpha-beta &  1.29 &  0.53 &  0.22 \\ \hline
          Minimax alpha-beta parallel & 1.05 & 0.25 & 0.10 \\ \hline
          \textbf{Dimension: 9x9} &    &    &     \\ \hline
          Minimax  &  1  &   1 &   1  \\ \hline
          Minimax alpha-beta &  1.06 &  1 &  0.35 \\ \hline
          Minimax alpha-beta parallel & 0.43 & 0.33 & 0.19  \\ \hline
     \end{tabular}
     \caption{Table with the values of relative move-time complexity compared to the baseline minimax algorithm}
     \label{tab:complexity}
 \end{table}

In the table \ref{tab:complexity}, we present the relative move time complexity of the different algorithms with reference to the minimax algorithm for the same depth. Here, we can see that, especially for depth 2 and depth 3, the time minimax algorithm when using the alpha beta pruning and further parallel implementation reduces significantly. This is significant especially when considering the large dimension of the Quoridor board as the move time complexity is significantly high. Here, optimization is required for running the algorithm in relatively manageable time.

Following, we also performed the simulations with the \gls{MCTS} agent. In the simulation, we consider a Quoridor board of dimension 5x5 and play against a Minimax agent. As described in earlier sections, the \gls{MCTS} algorithm requires multiple iterations through the game tree. More iterations provide a more reliable strategy at the cost of time complexity. This is even more prominent as we increase the dimension of the board due to the exponential increase in the state space and game tree complexity as described in section \ref{GameAnalysis}.

\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.7\linewidth]{../img/mcts_simulation_grid_search.png}
    \caption{Average time per move and win rates of MCTS agent with varying simulations for board size 5x5}
    \label{fig:mcts_simulations}
\end{figure}

In figure \ref{fig:mcts_simulations}, we show variation of the number of games won with the \gls{MCTS} agent againt the minimax agent varied with the number of game iterations. In the same figure, we also show the average time per move required when varying game simulations with the number of \gls{MCTS} simulations. For each number of varied \gls{MCTS} simulation, we performed 50 game simulations. We can see that after 90 simulations, the additional marginal number of game won does not increase, although the time per move increases. Hence, optimizing the number of simulations can be important to limiting the complexity of the \gls{MCTS} agent.

\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.7\linewidth]{../img/mcts_exploration_param_grid_search.png}
    \caption{Average time per move and win rates of MCTS agent with varying exploration parameter for board size 5x5}
    \label{fig:mcts_exp_simulations}
\end{figure}

Likewise, in figure \ref{fig:mcts_exp_simulations}, we consider find the optimal exploration parameter based on number of games won varied with different exploration parameters, In \gls{MCTS}, the exploration parameter is used in the exploration step to explore the nodes not visited yet. In addition, in the figure, we also show the average move time for different exploration parameters. In the figure, we see that with an optimal exploration parameter of 1.7 the win rate for the agent is high i.e., 46 wins out of 50 games.

We finally conduct an experiment for multiple agents, namely the random, semi-random, minimax, MCTS and the A-star agents against each other. We conduct the experiment for different board dimensions multiple number of times and present the results in the tables \ref{tab:agent_eval_3x3}, \ref{tab:agent_eval_5x5}, \ref{tab:agent_eval_7x7} and \ref{tab:agent_eval_9x9}. The values in the table represent the total win percentage of the agent in the row of the table when competing against the agent in the column of the table.

\begin{table}[!ht]
    \centering
     \begin{tabular}{|c|c|c|c|c|c|}\hline
                & Random & Semi-Random & A*  & Minimax & MCTS \\ \hline 
    Random      &        &    31.3     & 6.6 &   5.3   &  0   \\ \hline
    Semi-Random &   73.5 &             & 11.5&   10.2  & 0.1  \\ \hline
    A*          &   92.2 &    90.2     &     &   0     &  0   \\ \hline
    Minimax     &   92.4 &    88.9     & 1.5 &         &  0   \\ \hline
    MCTS        &   98   &    89.3     &  16 &   3     &      \\ \hline
     \end{tabular}
     \caption{Agent comparison for dimension 3x3}
     \label{tab:agent_eval_3x3}
 \end{table}

 In table \ref{tab:agent_eval_3x3}, we present the tournament result for the agents for the board dimension 3x3 and present the results. Firstly, we can observe that the random, as expected, does poorly against all the agents. However, in the 3x3 board, the random agent still wins a third of the time again semi-random agent and a few times against the even the A-star and the minimax agents. From the table, it can also be observed that the MCTS agent performs the best in the 3x3 board as it almost does not lose any games against the any other agents.

\begin{table}[!ht]
    \centering
     \begin{tabular}{|c|c|c|c|c|c|}\hline
                & Random & Semi-Random & A*  & Minimax & MCTS \\ \hline 
    Random      &        &    5.8      &  0  &   0     &   0  \\ \hline
    Semi-Random &   93.6 &             & 0.3 &   1.9   &  3.3 \\ \hline
    A*          &   100  &    99.7     &     &   1     & 73.3 \\ \hline
    Minimax     &   92.4 &    88.9     & 1.5 &         & 33.3 \\ \hline
    MCTS        &   100  &    100      & 56.7&   92    &      \\ \hline
     \end{tabular}
     \caption{Agent comparison for dimension 5x5}
     \label{tab:agent_eval_5x5}
 \end{table}

 In table \ref{tab:agent_eval_5x5}, we present the result of the tournament when considering a Quoridor board of dimension 5. Considering a board of larger dimension compared to before, we can see that the random agent performs poorly in a larger board. We can also see that the MCTS algorithm performs best against all the algorithms winning more than 90\% of its games except for the A-star agent where it wins more than half of its games. Another surprising factor is that the A-star algorithm which only considers the distance from the start position and end position as the main cost functions performs much better than the minimax algorithm. The conclusion here is that for 

\begin{table}[!ht]
    \centering
     \begin{tabular}{|c|c|c|c|c|c|}\hline
                & Random & Semi-Random & A*  & Minimax & MCTS \\ \hline 
    Random      &        &    2        &  0  &   0     &      \\ \hline
    Semi-Random &   99.6 &             & 0.2 &  0.7    &  0.2 \\ \hline
    A*          &   100  &    99.7     &     &   0     &      \\ \hline
    Minimax     &   100  &    99.3     & 100 &         & 26.6 \\ \hline
    MCTS        &        &             &     &         &      \\ \hline
     \end{tabular}
     \caption{Agent comparison for dimension 7x7}
     \label{tab:agent_eval_7x7}
 \end{table}
 
In table \ref{tab:agent_eval_7x7}, we illustrate the result of the tournament when considering a board of dimension 7x7. Here, the performance of the random agent is even more significantly worse compared to the previous boards as it fails to win any games against A-star, minimax and MCTS agents. Unlike the previous smaller boards, in this instance, the minimax algorithm performs much better against A-star algorithm as it wins almost all of its games. From this, we can conclude that the strategical play with the minimax algorithm for larger board is much more significant than the simple cost function oriented traversal strategy of the A-star algorithm.
 

\begin{table}[!ht]
    \centering
     \begin{tabular}{|c|c|c|c|c|c|}\hline
                & Random & Semi-Random & A*  & Minimax & MCTS \\ \hline 
    Random      &        &    0        &  0  &   0     &      \\ \hline
    Semi-Random &   100  &             &  0  &  0.1    &      \\ \hline
    A*          &   100  &    100      &     &  2.8    &      \\ \hline
    Minimax     &   100  &    100      & 100 &         &      \\ \hline
    MCTS        &        &             &     &         &      \\ \hline
     \end{tabular}
     \caption{Agent comparison for dimension 9x9}
     \label{tab:agent_eval_9x9}
 \end{table}

 In table \ref{tab:agent_eval_9x9}, we can see the result of the tournament when  considering the board of dimension 9. Here we can see that the random agent does not win any games against any other agents. Likewise the semi-random agent also performs poorly against all the other agents except for the random agent as it fails to win almost all the games. The A-star agent wins all the games against the random and the semi-random agents however fails to win any game against the minimax agent.

 From the above tournament simulation, we can conclude that the board dimension does have an effect on the strategy that might be used. For board of smaller dimensions such as 3 and 5, simpler strategies such as A-star and even random and semi-random in some cases can be enough to get some wins against more strategical agents such as the minimax agent. However, as the board dimension gets higher such as 7 and 9, the required strategy is very prominently visible as A-star, semi-random and random agents win almost no games against the minimax and MCTS agents.